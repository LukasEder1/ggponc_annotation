{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a11bef0-b53a-43b1-8115-bf2bd975b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../util')\n",
    "sys.path.insert(1, '../experiments')\n",
    "\n",
    "import os\n",
    "# Disable weights and biases (if installed)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1cf53c-d388-4283-9cea-3ae11f4a4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline, DataCollatorForTokenClassification, EarlyStoppingCallback, trainer_utils\n",
    "\n",
    "from huggingface_utils import load_custom_dataset, LabelAligner, compute_metrics, eval_on_test_set\n",
    "from run_experiment import get_train_args\n",
    "from convert_annotations import entity_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777251b-f8c6-448d-9c24-9bdf541890fa",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7ff9c5-934c-45b2-91a6-01cf12db84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'fine' # Change to 'value' to look at high-level entity classes only\n",
    "spans = 'short' # Change to 'long' to consider long spans induced by specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0880b43-b6ee-4b3e-b46a-445ff74fb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_files = {\n",
    "    ('coarse' , 'short') : '01_ggponc_coarse_short.yaml',\n",
    "    ('fine', 'short') : '02_ggponc_fine_short.yaml',\n",
    "    ('coarse' , 'long' ) : '03_ggponc_coarsee_long.yaml',\n",
    "    ('fine', 'long' ) : '04_ggponc_fine_long.yaml'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca90fa-6e16-41c2-8d1e-164ce4a4222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=Path('..') / 'experiments', job_name='foo')\n",
    "config = compose(config_name=config_files[(level, spans)], overrides=['cuda=0', 'link=false'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fba68f4-1216-4902-8d1c-c881b9bcacd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = config['train_dataset']\n",
    "dev_file = config['dev_dataset']\n",
    "test_file = config['test_dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45224845-4d54-4877-8d46-5157eb290b7d",
   "metadata": {},
   "source": [
    "# Setup IOB-encoded dataset with train / dev / test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75372f49-e40d-4c98-9479-ea6d03b0b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-aa369a42bd5d02d6\n",
      "WARNING:datasets.builder:Reusing dataset json (/dhc/home/florian.borchert/.cache/huggingface/datasets/json/default-aa369a42bd5d02d6/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b464d386fdb847cbb5067eaae34c07da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /dhc/home/florian.borchert/.cache/huggingface/datasets/json/default-aa369a42bd5d02d6/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-86b21d76916a7285.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /dhc/home/florian.borchert/.cache/huggingface/datasets/json/default-aa369a42bd5d02d6/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-2b6984a87425f4cd.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /dhc/home/florian.borchert/.cache/huggingface/datasets/json/default-aa369a42bd5d02d6/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-9ea906c9efcf2cc2.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /dhc/home/florian.borchert/.cache/huggingface/datasets/json/default-aa369a42bd5d02d6/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-17c193210c4f488b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /dhc/home/florian.borchert/.cache/huggingface/datasets/json/default-aa369a42bd5d02d6/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-e422dab7fe85078b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /dhc/home/florian.borchert/.cache/huggingface/datasets/json/default-aa369a42bd5d02d6/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-eb50fdab135d13bb.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset, tags = load_custom_dataset(train=train_file, dev=dev_file, test=test_file, tag_strings=config['task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167acfe0-7690-4082-878c-cabc210eed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['base_model_checkpoint'])\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0fbcf2e-5b4a-442b-a18f-d5992f0a5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_aligner = LabelAligner(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94d16e1-63c9-4024-992d-1141b9637b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d42f9eae7048edba455adfc6198909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69560915c63c492c96722b6c93f290ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b57ae0919ae4493a8ac46844b85d4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda e: label_aligner.tokenize_and_align_labels(e, config['label_all_tokens']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37f9a69-b199-4974-8a1f-d07c487fff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-Other_Finding',\n",
       " 2: 'I-Other_Finding',\n",
       " 3: 'B-Diagnosis_or_Pathology',\n",
       " 4: 'I-Diagnosis_or_Pathology',\n",
       " 5: 'B-Therapeutic',\n",
       " 6: 'I-Therapeutic',\n",
       " 7: 'B-Diagnostic',\n",
       " 8: 'I-Diagnostic',\n",
       " 9: 'B-Nutrient_or_Body_Substance',\n",
       " 10: 'I-Nutrient_or_Body_Substance',\n",
       " 11: 'B-External_Substance',\n",
       " 12: 'I-External_Substance',\n",
       " 13: 'B-Clinical_Drug',\n",
       " 14: 'I-Clinical_Drug'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = dict(enumerate(tags))\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08fcd242-41e0-420e-9fbf-2f9cdc125916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_tags', 'attention_mask', 'fname', 'input_ids', 'labels', 'offset_mapping', 'sentence_id', 'special_tokens_mask', 'tags', 'token_type_ids', 'tokens'],\n",
       "        num_rows: 23528\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['_tags', 'attention_mask', 'fname', 'input_ids', 'labels', 'offset_mapping', 'sentence_id', 'special_tokens_mask', 'tags', 'token_type_ids', 'tokens'],\n",
       "        num_rows: 4655\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_tags', 'attention_mask', 'fname', 'input_ids', 'labels', 'offset_mapping', 'sentence_id', 'special_tokens_mask', 'tags', 'token_type_ids', 'tokens'],\n",
       "        num_rows: 4826\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7662b9-273a-4285-8ec0-1f0354153c1a",
   "metadata": {},
   "source": [
    "# Configure and train ðŸ¤— token classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89d59f4-5267-4023-88ab-e3ef66fa044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_experiment import get_train_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df4091b-00c3-43c3-8407-1363f1443881",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 10 # Remove this line to train for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c73ddba2-4fd5-46a2-8dcd-ff737f2a0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['num_train_epochs'] = num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef69d25f-e0cb-424f-8f82-87fd3716b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:run_experiment:ner_baseline\n"
     ]
    }
   ],
   "source": [
    "training_args = get_train_args(cp_path='../ner_results', run_name='ner_baseline', report_to=[], **config, resume_from_checkpoint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4c0334a-3101-4d4d-a245-d91c0d7bb4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_find_unused_parameters=None,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=IntervalStrategy.EPOCH,\n",
       "fp16=True,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=True,\n",
       "greater_is_better=True,\n",
       "group_by_length=False,\n",
       "hub_model_id=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.05,\n",
       "learning_rate=1e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=-1,\n",
       "log_level_replica=-1,\n",
       "log_on_each_node=True,\n",
       "logging_dir=../ner_results/runs/Dec13_14-28-54_geras,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=eval_overall_f1,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=10,\n",
       "output_dir=../ner_results,\n",
       "overwrite_output_dir=True,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=32,\n",
       "per_device_train_batch_size=32,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=../ner_results,\n",
       "save_on_each_node=False,\n",
       "save_steps=10000,\n",
       "save_strategy=IntervalStrategy.EPOCH,\n",
       "save_total_limit=2,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_legacy_prediction_loop=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a4e392-7ac5-4276-aea0-09741f439246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/deepset/gbert-base/resolve/main/config.json from cache at /dhc/home/florian.borchert/.cache/huggingface/transformers/0f9d6c73cd85ab98cecc6866492c84f23e72bbaf2240a24da0e5d5e3b8810707.080f0bd0794ab07ca509487675f6cb88cfbdc04fc142b21be92212223e82cb14\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-Other_Finding\",\n",
      "    \"2\": \"I-Other_Finding\",\n",
      "    \"3\": \"B-Diagnosis_or_Pathology\",\n",
      "    \"4\": \"I-Diagnosis_or_Pathology\",\n",
      "    \"5\": \"B-Therapeutic\",\n",
      "    \"6\": \"I-Therapeutic\",\n",
      "    \"7\": \"B-Diagnostic\",\n",
      "    \"8\": \"I-Diagnostic\",\n",
      "    \"9\": \"B-Nutrient_or_Body_Substance\",\n",
      "    \"10\": \"I-Nutrient_or_Body_Substance\",\n",
      "    \"11\": \"B-External_Substance\",\n",
      "    \"12\": \"I-External_Substance\",\n",
      "    \"13\": \"B-Clinical_Drug\",\n",
      "    \"14\": \"I-Clinical_Drug\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/deepset/gbert-base/resolve/main/pytorch_model.bin from cache at /dhc/home/florian.borchert/.cache/huggingface/transformers/be505b3d2fb2c6041a26dc305c082e586208362a967e248c8b31de9c50fa498a.214cb6009589cc362d53549c3062e1f4cc563e1b95b5d32185c48a18cfb03bf9\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        config['base_model_checkpoint'],\n",
    "        num_labels=len(tags), \n",
    "        id2label=id2label,\n",
    "    )\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "tr = Trainer(\n",
    "    args=training_args,\n",
    "    model_init=model_init,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics(tags, True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f96996-6978-444c-bfdf-ba0f5ca7b3c4",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0264e29f-f90a-43d9-81cb-909d4ebd10eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/deepset/gbert-base/resolve/main/config.json from cache at /dhc/home/florian.borchert/.cache/huggingface/transformers/0f9d6c73cd85ab98cecc6866492c84f23e72bbaf2240a24da0e5d5e3b8810707.080f0bd0794ab07ca509487675f6cb88cfbdc04fc142b21be92212223e82cb14\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-Other_Finding\",\n",
      "    \"2\": \"I-Other_Finding\",\n",
      "    \"3\": \"B-Diagnosis_or_Pathology\",\n",
      "    \"4\": \"I-Diagnosis_or_Pathology\",\n",
      "    \"5\": \"B-Therapeutic\",\n",
      "    \"6\": \"I-Therapeutic\",\n",
      "    \"7\": \"B-Diagnostic\",\n",
      "    \"8\": \"I-Diagnostic\",\n",
      "    \"9\": \"B-Nutrient_or_Body_Substance\",\n",
      "    \"10\": \"I-Nutrient_or_Body_Substance\",\n",
      "    \"11\": \"B-External_Substance\",\n",
      "    \"12\": \"I-External_Substance\",\n",
      "    \"13\": \"B-Clinical_Drug\",\n",
      "    \"14\": \"I-Clinical_Drug\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/deepset/gbert-base/resolve/main/pytorch_model.bin from cache at /dhc/home/florian.borchert/.cache/huggingface/transformers/be505b3d2fb2c6041a26dc305c082e586208362a967e248c8b31de9c50fa498a.214cb6009589cc362d53549c3062e1f4cc563e1b95b5d32185c48a18cfb03bf9\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 23528\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7360\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7360' max='7360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7360/7360 21:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Clinical Drug Precision</th>\n",
       "      <th>Clinical Drug Recall</th>\n",
       "      <th>Clinical Drug F1</th>\n",
       "      <th>Clinical Drug Number</th>\n",
       "      <th>Diagnosis Or Pathology Precision</th>\n",
       "      <th>Diagnosis Or Pathology Recall</th>\n",
       "      <th>Diagnosis Or Pathology F1</th>\n",
       "      <th>Diagnosis Or Pathology Number</th>\n",
       "      <th>Diagnostic Precision</th>\n",
       "      <th>Diagnostic Recall</th>\n",
       "      <th>Diagnostic F1</th>\n",
       "      <th>Diagnostic Number</th>\n",
       "      <th>External Substance Precision</th>\n",
       "      <th>External Substance Recall</th>\n",
       "      <th>External Substance F1</th>\n",
       "      <th>External Substance Number</th>\n",
       "      <th>Nutrient Or Body Substance Precision</th>\n",
       "      <th>Nutrient Or Body Substance Recall</th>\n",
       "      <th>Nutrient Or Body Substance F1</th>\n",
       "      <th>Nutrient Or Body Substance Number</th>\n",
       "      <th>Other Finding Precision</th>\n",
       "      <th>Other Finding Recall</th>\n",
       "      <th>Other Finding F1</th>\n",
       "      <th>Other Finding Number</th>\n",
       "      <th>Therapeutic Precision</th>\n",
       "      <th>Therapeutic Recall</th>\n",
       "      <th>Therapeutic F1</th>\n",
       "      <th>Therapeutic Number</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>0.438262</td>\n",
       "      <td>0.757522</td>\n",
       "      <td>0.887967</td>\n",
       "      <td>0.817574</td>\n",
       "      <td>964</td>\n",
       "      <td>0.805428</td>\n",
       "      <td>0.876629</td>\n",
       "      <td>0.839522</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.795750</td>\n",
       "      <td>0.822285</td>\n",
       "      <td>0.808800</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.424893</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>233</td>\n",
       "      <td>0.720899</td>\n",
       "      <td>0.687240</td>\n",
       "      <td>0.703667</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.819464</td>\n",
       "      <td>0.869539</td>\n",
       "      <td>0.843759</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.782637</td>\n",
       "      <td>0.815927</td>\n",
       "      <td>0.798935</td>\n",
       "      <td>0.955793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.427277</td>\n",
       "      <td>0.767462</td>\n",
       "      <td>0.900415</td>\n",
       "      <td>0.828640</td>\n",
       "      <td>964</td>\n",
       "      <td>0.856530</td>\n",
       "      <td>0.867507</td>\n",
       "      <td>0.861983</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.798044</td>\n",
       "      <td>0.862952</td>\n",
       "      <td>0.829230</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57</td>\n",
       "      <td>0.550562</td>\n",
       "      <td>0.630901</td>\n",
       "      <td>0.588000</td>\n",
       "      <td>233</td>\n",
       "      <td>0.742216</td>\n",
       "      <td>0.708986</td>\n",
       "      <td>0.725221</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.830199</td>\n",
       "      <td>0.890315</td>\n",
       "      <td>0.859207</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.805809</td>\n",
       "      <td>0.833186</td>\n",
       "      <td>0.819269</td>\n",
       "      <td>0.959097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392200</td>\n",
       "      <td>0.427908</td>\n",
       "      <td>0.760908</td>\n",
       "      <td>0.904564</td>\n",
       "      <td>0.826540</td>\n",
       "      <td>964</td>\n",
       "      <td>0.827711</td>\n",
       "      <td>0.895308</td>\n",
       "      <td>0.860184</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.826259</td>\n",
       "      <td>0.847092</td>\n",
       "      <td>0.836546</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>57</td>\n",
       "      <td>0.590308</td>\n",
       "      <td>0.575107</td>\n",
       "      <td>0.582609</td>\n",
       "      <td>233</td>\n",
       "      <td>0.755684</td>\n",
       "      <td>0.712184</td>\n",
       "      <td>0.733289</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.830561</td>\n",
       "      <td>0.900092</td>\n",
       "      <td>0.863930</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.805866</td>\n",
       "      <td>0.841951</td>\n",
       "      <td>0.823514</td>\n",
       "      <td>0.959740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.385100</td>\n",
       "      <td>0.423742</td>\n",
       "      <td>0.767563</td>\n",
       "      <td>0.918050</td>\n",
       "      <td>0.836089</td>\n",
       "      <td>964</td>\n",
       "      <td>0.842148</td>\n",
       "      <td>0.892268</td>\n",
       "      <td>0.866484</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.849269</td>\n",
       "      <td>0.827166</td>\n",
       "      <td>0.838072</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>57</td>\n",
       "      <td>0.561702</td>\n",
       "      <td>0.566524</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>233</td>\n",
       "      <td>0.762638</td>\n",
       "      <td>0.728494</td>\n",
       "      <td>0.745175</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.861507</td>\n",
       "      <td>0.887565</td>\n",
       "      <td>0.874342</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.821647</td>\n",
       "      <td>0.839234</td>\n",
       "      <td>0.830347</td>\n",
       "      <td>0.961441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.424664</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.914938</td>\n",
       "      <td>0.826617</td>\n",
       "      <td>964</td>\n",
       "      <td>0.851020</td>\n",
       "      <td>0.897046</td>\n",
       "      <td>0.873427</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.828338</td>\n",
       "      <td>0.865392</td>\n",
       "      <td>0.846460</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>57</td>\n",
       "      <td>0.612440</td>\n",
       "      <td>0.549356</td>\n",
       "      <td>0.579186</td>\n",
       "      <td>233</td>\n",
       "      <td>0.763328</td>\n",
       "      <td>0.718900</td>\n",
       "      <td>0.740448</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.841940</td>\n",
       "      <td>0.901619</td>\n",
       "      <td>0.870758</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.817104</td>\n",
       "      <td>0.847863</td>\n",
       "      <td>0.832200</td>\n",
       "      <td>0.961695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.424598</td>\n",
       "      <td>0.766696</td>\n",
       "      <td>0.917012</td>\n",
       "      <td>0.835144</td>\n",
       "      <td>964</td>\n",
       "      <td>0.859590</td>\n",
       "      <td>0.893571</td>\n",
       "      <td>0.876251</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.834707</td>\n",
       "      <td>0.864579</td>\n",
       "      <td>0.849381</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.192982</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>57</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.652361</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>233</td>\n",
       "      <td>0.760905</td>\n",
       "      <td>0.741925</td>\n",
       "      <td>0.751295</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.847385</td>\n",
       "      <td>0.905897</td>\n",
       "      <td>0.875665</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.821377</td>\n",
       "      <td>0.854250</td>\n",
       "      <td>0.837491</td>\n",
       "      <td>0.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.426531</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.912863</td>\n",
       "      <td>0.841300</td>\n",
       "      <td>964</td>\n",
       "      <td>0.851495</td>\n",
       "      <td>0.902911</td>\n",
       "      <td>0.876450</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.839477</td>\n",
       "      <td>0.861326</td>\n",
       "      <td>0.850261</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>57</td>\n",
       "      <td>0.573222</td>\n",
       "      <td>0.587983</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>233</td>\n",
       "      <td>0.757429</td>\n",
       "      <td>0.749920</td>\n",
       "      <td>0.753656</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.853503</td>\n",
       "      <td>0.900703</td>\n",
       "      <td>0.876468</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.820758</td>\n",
       "      <td>0.855949</td>\n",
       "      <td>0.837984</td>\n",
       "      <td>0.962627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.428078</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.914938</td>\n",
       "      <td>0.840801</td>\n",
       "      <td>964</td>\n",
       "      <td>0.860499</td>\n",
       "      <td>0.899001</td>\n",
       "      <td>0.879329</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.838735</td>\n",
       "      <td>0.862952</td>\n",
       "      <td>0.850671</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>57</td>\n",
       "      <td>0.578723</td>\n",
       "      <td>0.583691</td>\n",
       "      <td>0.581197</td>\n",
       "      <td>233</td>\n",
       "      <td>0.746629</td>\n",
       "      <td>0.761433</td>\n",
       "      <td>0.753958</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.853877</td>\n",
       "      <td>0.901619</td>\n",
       "      <td>0.877099</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.820593</td>\n",
       "      <td>0.857784</td>\n",
       "      <td>0.838776</td>\n",
       "      <td>0.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.429315</td>\n",
       "      <td>0.778073</td>\n",
       "      <td>0.912863</td>\n",
       "      <td>0.840095</td>\n",
       "      <td>964</td>\n",
       "      <td>0.857172</td>\n",
       "      <td>0.900738</td>\n",
       "      <td>0.878416</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.838155</td>\n",
       "      <td>0.871899</td>\n",
       "      <td>0.854694</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>57</td>\n",
       "      <td>0.609442</td>\n",
       "      <td>0.609442</td>\n",
       "      <td>0.609442</td>\n",
       "      <td>233</td>\n",
       "      <td>0.751517</td>\n",
       "      <td>0.752478</td>\n",
       "      <td>0.751997</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.856392</td>\n",
       "      <td>0.896425</td>\n",
       "      <td>0.875952</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.821973</td>\n",
       "      <td>0.857104</td>\n",
       "      <td>0.839171</td>\n",
       "      <td>0.962880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.429483</td>\n",
       "      <td>0.778565</td>\n",
       "      <td>0.911826</td>\n",
       "      <td>0.839943</td>\n",
       "      <td>964</td>\n",
       "      <td>0.860828</td>\n",
       "      <td>0.898784</td>\n",
       "      <td>0.879396</td>\n",
       "      <td>4604</td>\n",
       "      <td>0.836583</td>\n",
       "      <td>0.872306</td>\n",
       "      <td>0.854071</td>\n",
       "      <td>2459</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>57</td>\n",
       "      <td>0.589520</td>\n",
       "      <td>0.579399</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>233</td>\n",
       "      <td>0.746147</td>\n",
       "      <td>0.758555</td>\n",
       "      <td>0.752299</td>\n",
       "      <td>3127</td>\n",
       "      <td>0.856434</td>\n",
       "      <td>0.896731</td>\n",
       "      <td>0.876119</td>\n",
       "      <td>3273</td>\n",
       "      <td>0.821312</td>\n",
       "      <td>0.857308</td>\n",
       "      <td>0.838924</td>\n",
       "      <td>0.962727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "/dhc/home/florian.borchert/conda3/envs/ggponc/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../ner_results/checkpoint-736\n",
      "Configuration saved in ../ner_results/checkpoint-736/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-736/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-736/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-736/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-3680] due to args.save_total_limit\n",
      "Deleting older checkpoint [../ner_results/checkpoint-4416] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "/dhc/home/florian.borchert/conda3/envs/ggponc/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ../ner_results/checkpoint-1472\n",
      "Configuration saved in ../ner_results/checkpoint-1472/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-1472/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-1472/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-1472/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-5152] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-2208\n",
      "Configuration saved in ../ner_results/checkpoint-2208/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-2208/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-2208/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-2208/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-736] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-2944\n",
      "Configuration saved in ../ner_results/checkpoint-2944/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-2944/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-2944/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-2944/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-1472] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-3680\n",
      "Configuration saved in ../ner_results/checkpoint-3680/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-3680/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-3680/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-3680/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-2208] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-4416\n",
      "Configuration saved in ../ner_results/checkpoint-4416/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-4416/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-4416/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-4416/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-2944] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-5152\n",
      "Configuration saved in ../ner_results/checkpoint-5152/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-5152/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-5152/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-5152/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-3680] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-5888\n",
      "Configuration saved in ../ner_results/checkpoint-5888/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-5888/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-5888/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-5888/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-4416] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-6624\n",
      "Configuration saved in ../ner_results/checkpoint-6624/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-6624/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-6624/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-6624/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-5152] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4655\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../ner_results/checkpoint-7360\n",
      "Configuration saved in ../ner_results/checkpoint-7360/config.json\n",
      "Model weights saved in ../ner_results/checkpoint-7360/pytorch_model.bin\n",
      "tokenizer config file saved in ../ner_results/checkpoint-7360/tokenizer_config.json\n",
      "Special tokens file saved in ../ner_results/checkpoint-7360/special_tokens_map.json\n",
      "Deleting older checkpoint [../ner_results/checkpoint-5888] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../ner_results/checkpoint-6624 (score: 0.8391710740777699).\n"
     ]
    }
   ],
   "source": [
    "train_result = tr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fc922-1ab1-47e2-badb-d52f43999213",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9d4552d-4ac5-4e5f-9f2e-f794cd91ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tr.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8633db62-4834-4933-9ee4-0b4cde6f1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.token_classification import AggregationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ca5129b-a476-48f3-b12f-896d63f87d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"ner\", model, tokenizer=tokenizer, device=0, aggregation_strategy=AggregationStrategy.FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69887dc6-3020-4548-b704-421a394f9bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Therapeutic',\n",
       "  'score': 0.96289647,\n",
       "  'word': 'Therapie',\n",
       "  'start': 0,\n",
       "  'end': 8},\n",
       " {'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.8416674,\n",
       "  'word': 'Interferon Alpha 2b',\n",
       "  'start': 25,\n",
       "  'end': 44},\n",
       " {'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.96495634,\n",
       "  'word': 'Ribavirin',\n",
       "  'start': 45,\n",
       "  'end': 54},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.88501763,\n",
       "  'word': 'Beendigung',\n",
       "  'start': 70,\n",
       "  'end': 80},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.9672504,\n",
       "  'word': 'Therapie',\n",
       "  'start': 85,\n",
       "  'end': 93},\n",
       " {'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.96899027,\n",
       "  'word': 'Nivolumab',\n",
       "  'start': 98,\n",
       "  'end': 107},\n",
       " {'entity_group': 'Diagnosis_or_Pathology',\n",
       "  'score': 0.9015603,\n",
       "  'word': 'Oesophagusvarizen',\n",
       "  'start': 145,\n",
       "  'end': 162},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.9395949,\n",
       "  'word': 'FortfÃ¼hren',\n",
       "  'start': 170,\n",
       "  'end': 180},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.9715175,\n",
       "  'word': 'Chemotherapie',\n",
       "  'start': 185,\n",
       "  'end': 198},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.9674991,\n",
       "  'word': 'FOLFOX',\n",
       "  'start': 203,\n",
       "  'end': 209},\n",
       " {'entity_group': 'Diagnosis_or_Pathology',\n",
       "  'score': 0.9571444,\n",
       "  'word': 'Veraenderungen',\n",
       "  'start': 247,\n",
       "  'end': 261}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Therapie mit pegyliertem Interferon Alpha 2b/Ribavirin, non response. Beendigung der Therapie mit Nivolumab. 1-0-1, derzeit pausiert 23.03.2035: Oesophagusvarizen II. 4. FortfÃ¼hren der Chemotherapie mit FOLFOX. Groessenprogrediente mikronodulaere Veraenderungen der Lunge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03794b3f-6795-47fe-b5ff-a676c08e4576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.97322357,\n",
       "  'word': 'Sotorasib',\n",
       "  'start': 0,\n",
       "  'end': 9},\n",
       " {'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.94508743,\n",
       "  'word': 'AntikÃ¶rper',\n",
       "  'start': 31,\n",
       "  'end': 41},\n",
       " {'entity_group': 'Nutrient_or_Body_Substance',\n",
       "  'score': 0.93117774,\n",
       "  'word': 'Wachstumsfaktorrezeptor',\n",
       "  'start': 69,\n",
       "  'end': 92},\n",
       " {'entity_group': 'Nutrient_or_Body_Substance',\n",
       "  'score': 0.9528037,\n",
       "  'word': 'EGFR',\n",
       "  'start': 94,\n",
       "  'end': 98},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.9749288,\n",
       "  'word': 'Therapie',\n",
       "  'start': 128,\n",
       "  'end': 136},\n",
       " {'entity_group': 'Diagnosis_or_Pathology',\n",
       "  'score': 0.945694,\n",
       "  'word': 'Karzinoms',\n",
       "  'start': 172,\n",
       "  'end': 181},\n",
       " {'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.9710377,\n",
       "  'word': 'Irinotecan',\n",
       "  'start': 195,\n",
       "  'end': 205},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.95461553,\n",
       "  'word': 'FOLFOX',\n",
       "  'start': 230,\n",
       "  'end': 236},\n",
       " {'entity_group': 'Diagnosis_or_Pathology',\n",
       "  'score': 0.73630005,\n",
       "  'word': 'Versagen',\n",
       "  'start': 254,\n",
       "  'end': 262},\n",
       " {'entity_group': 'Therapeutic',\n",
       "  'score': 0.97218,\n",
       "  'word': 'Behandlung',\n",
       "  'start': 269,\n",
       "  'end': 279},\n",
       " {'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.97018844,\n",
       "  'word': 'Oxaliplatin',\n",
       "  'start': 284,\n",
       "  'end': 295},\n",
       " {'entity_group': 'Clinical_Drug',\n",
       "  'score': 0.9696947,\n",
       "  'word': 'Irinotecan',\n",
       "  'start': 300,\n",
       "  'end': 310}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Sotorasib ist ein monoklonaler AntikÃ¶rper, der gegen den epidermalen Wachstumsfaktorrezeptor (EGFR) gerichtet ist und dient zur Therapie des fortgeschrittenen kolorektalen Karzinoms zusammen mit Irinotecan oder in Kombination mit FOLFOX bzw. allein nach Versagen einer Behandlung mit Oxaliplatin und Irinotecan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8d4df77-71ae-41f5-9833-ee7eb2e8ac13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: offset_mapping, tags, fname, _tags, sentence_id, special_tokens_mask, tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4826\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/151 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4826it [00:03, 1559.24it/s]\n"
     ]
    }
   ],
   "source": [
    "test_metrics = eval_on_test_set(dataset[\"test\"], tr, tokenizer, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4065841a-2cf2-4bb0-876a-d1f6b2118cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1: 0.86\n",
      " P: 0.84\n",
      " R: 0.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "F1: {test_metrics[\"test/overall_f1\"]:.2f}\n",
    " P: {test_metrics[\"test/overall_precision\"]:.2f}\n",
    " R: {test_metrics[\"test/overall_recall\"]:.2f}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca141156-6c0f-44f4-8f20-ae8cabe83089",
   "metadata": {},
   "source": [
    "### Detailed analysis of model performance\n",
    "\n",
    "See notebook: [03_NER_Analysis](03_NER_Analysis.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2fd7f-73af-4ab4-8f9b-427806acf1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
